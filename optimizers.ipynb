{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a866f144",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0622e",
   "metadata": {},
   "source": [
    "1. Optimization algorithms play a crucial role in artificial neural networks (ANNs) for training and fine-tuning the network's parameters to minimize the difference between the actual output and the desired output. Here's why they are necessary:\n",
    "\n",
    "Parameter Optimization: ANNs typically contain a large number of parameters (weights and biases) that need to be adjusted during the training process to minimize the loss function. Optimization algorithms determine how these parameters should be updated to reduce the error between the predicted output and the actual output.\n",
    "\n",
    "Convergence: ANNs are often trained using gradient-based optimization techniques. Optimization algorithms help in finding the optimal set of parameters by iteratively updating them in the direction that minimizes the loss function. Without efficient optimization algorithms, the training process may not converge or may take an impractically long time to converge.\n",
    "\n",
    "Efficiency: Training ANNs involves processing large amounts of data through numerous iterations. Optimization algorithms help in making the training process computationally efficient by ensuring that the parameters are updated in an effective manner, thus reducing the time and computational resources required for training.\n",
    "\n",
    "Generalization: Good optimization algorithms help ANNs generalize well to unseen data. They prevent overfitting by finding the optimal set of parameters that capture the underlying patterns in the training data without memorizing noise or irrelevant details.\n",
    "\n",
    "Flexibility: Different optimization algorithms offer different trade-offs in terms of convergence speed, robustness, and memory requirements. Having a variety of optimization algorithms allows practitioners to choose the most suitable one based on the characteristics of the problem and the available computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3737bb0",
   "metadata": {},
   "source": [
    "2.  Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n",
    "Basic Concept of Gradient Descent\n",
    "Gradient descent involves three key steps:\n",
    "\n",
    "Initialization: Start with an initial point for the parameters you're trying to optimize. This could be random or based on some heuristic.\n",
    "Compute Gradient: Evaluate the gradient of the cost function at the current point.\n",
    "Update Parameters: Adjust the parameters in the direction opposite to the gradient of the function at that point, scaled by a learning rate.\n",
    "Mathematically, the parameter update rule is:\n",
    "�\n",
    ":\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "∇\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "θ:=θ−α∇J(θ)\n",
    "where:\n",
    "\n",
    "�\n",
    "θ represents the parameters,\n",
    "�\n",
    "α is the learning rate,\n",
    "∇\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "∇J(θ) is the gradient of the cost function \n",
    "�\n",
    "J with respect to \n",
    "�\n",
    "θ.\n",
    "Variants of Gradient Descent\n",
    "Batch Gradient Descent: The true gradient of the cost function is calculated from the entire dataset. This is computationally expensive and slow with large datasets but provides a stable error gradient and a stable convergence.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): The gradient of the cost function is estimated each time with a randomly selected subset of data (often just one instance). This can lead to rapid convergence since the update is made after computing the gradient on just one or a few training examples, but the path to convergence can be noisy.\n",
    "\n",
    "Mini-batch Gradient Descent: This is a compromise between batch and stochastic gradient descent where the update is performed on a small, random subset of the data. This variant aims to balance the efficiency of stochastic gradient descent with the stability of batch gradient descent.\n",
    "\n",
    "Differences and Trade-offs\n",
    "Convergence Speed:\n",
    "\n",
    "Batch Gradient Descent converges smoothly but can be very slow with large datasets.\n",
    "Stochastic Gradient Descent often converges much faster than batch gradient descent but can oscillate around the minimum.\n",
    "Mini-batch Gradient Descent provides a balance, often converging faster than batch gradient descent and with less oscillation than SGD.\n",
    "Memory Requirements:\n",
    "\n",
    "Batch Gradient Descent requires significant memory for large datasets since it processes the entire dataset at once.\n",
    "Stochastic and Mini-batch Gradient Descent are much more memory efficient since they require only a single or a small batch of examples to compute the gradient at each step.\n",
    "Stability and Accuracy:\n",
    "\n",
    "Batch Gradient Descent provides the most stable convergence to the true minimum but can be impractical for very large datasets.\n",
    "Stochastic Gradient Descent can overshoot due to its high variance in the gradient estimation, leading to a less stable convergence.\n",
    "Mini-batch Gradient Descent aims to reduce the variance in the gradient estimation, leading to a more stable convergence than SGD, albeit with potentially slower convergence than using the entire dataset.\n",
    "Conclusion\n",
    "The choice among these variants depends on the specific requirements of the task, including the size of the dataset, the computational resources available, and the desired balance between speed and accuracy of convergence. Batch gradient descent is simple and stable but can be impractical for large datasets. Stochastic gradient descent and its mini-batch variant offer more practical and flexible approaches that can handle large datasets more efficiently but introduce additional hyperparameters (like the batch size) that need to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b97a94",
   "metadata": {},
   "source": [
    "3.  Traditional gradient descent optimization methods, such as vanilla gradient descent, suffer from several challenges that can hinder their effectiveness in training neural networks:\n",
    "\n",
    "Slow Convergence: Gradient descent updates parameters by taking steps proportional to the gradient of the loss function. However, in high-dimensional spaces typical of neural networks, this approach can lead to slow convergence. The updates may oscillate or take small steps, especially in regions where the gradient is small, leading to slow progress towards the optimum.\n",
    "\n",
    "Local Minima and Plateaus: Gradient descent can get stuck in local minima or plateaus (regions where the gradient is close to zero). These regions can trap the optimization process, preventing it from reaching the global minimum of the loss function. Additionally, saddle points, where the gradient is zero but not a minimum, can further slow down convergence.\n",
    "\n",
    "Sensitivity to Learning Rate: The learning rate parameter in gradient descent determines the size of the steps taken during parameter updates. Choosing an appropriate learning rate can be challenging, as a too small learning rate can lead to slow convergence, while a too large learning rate can cause oscillations or divergence.\n",
    "\n",
    "Modern optimization algorithms address these challenges through various techniques:\n",
    "\n",
    "Momentum: Momentum-based optimization methods, such as Momentum, Nesterov Accelerated Gradient (NAG), and Adam, introduce a momentum term that accelerates the parameter updates in the direction of persistent gradients. This helps in overcoming plateaus and shallow regions and accelerates convergence.\n",
    "\n",
    "Adaptive Learning Rates: Algorithms like Adagrad, RMSProp, and Adam adapt the learning rate for each parameter based on their past gradients. This allows for faster convergence by automatically adjusting the learning rate according to the gradient magnitudes, mitigating the need for manual tuning.\n",
    "\n",
    "Second-Order Methods: Newton's Method and variants like Conjugate Gradient and L-BFGS compute second-order information (Hessian matrix) in addition to gradients. This allows for more accurate and efficient updates, especially in regions with complex curvature, leading to faster convergence.\n",
    "\n",
    "Randomness: Stochastic Gradient Descent (SGD) and its variants introduce randomness by randomly sampling batches of data for each parameter update. This randomness helps escape local minima and plateaus and can lead to faster convergence, especially in large-scale datasets.\n",
    "\n",
    "Normalization Techniques: Batch normalization and Layer normalization normalize the inputs to each layer during training, reducing internal covariate shift and allowing for smoother optimization trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28dec77",
   "metadata": {},
   "source": [
    "4.  Gradient descent is a first-order optimization algorithm used to minimize a function by iteratively moving in the direction of the steepest descent of the function. The basic idea is to update the parameters of a model in the opposite direction of the gradient of the loss function with respect to those parameters. This process is repeated until convergence or until a stopping criterion is met.\n",
    "\n",
    "Here's a breakdown of the gradient descent algorithm:\n",
    "\n",
    "Initialization: Initialize the parameters (weights and biases) of the model randomly or with some predefined values.\n",
    "\n",
    "Compute Gradient: Calculate the gradient of the loss function with respect to the parameters using techniques like backpropagation in neural networks.\n",
    "\n",
    "Update Parameters: Adjust the parameters in the opposite direction of the gradient by multiplying the gradient with a small scalar value called the learning rate (α). This step is represented by the following equation:\n",
    "�\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "⋅\n",
    "∇\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "θ=θ−α⋅∇J(θ)\n",
    "where \n",
    "�\n",
    "θ represents the parameters and \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "J(θ) is the loss function.\n",
    "\n",
    "Repeat: Iterate steps 2 and 3 until convergence or until a predetermined number of iterations is reached.\n",
    "\n",
    "Gradient descent has several variants designed to improve convergence speed and overcome limitations. Some popular variants include:\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Instead of computing the gradient using the entire dataset, SGD computes the gradient using only one random data point at a time. This can speed up computation and helps in escaping local minima, but it introduces more variance in the parameter updates.\n",
    "\n",
    "Mini-batch Gradient Descent: Mini-batch gradient descent combines the advantages of batch gradient descent and SGD by computing the gradient using a small random subset of the dataset (mini-batch). It strikes a balance between the stability of batch gradient descent and the efficiency of SGD.\n",
    "\n",
    "Momentum: Momentum is a technique that helps accelerate gradient descent in the relevant direction and dampens oscillations. It introduces a new hyperparameter called momentum (β) which determines the contribution of the past update directions to the current update. The update rule with momentum is given by:\n",
    "�\n",
    "=\n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    ")\n",
    "⋅\n",
    "∇\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "v=β⋅v+(1−β)⋅∇J(θ)\n",
    "�\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "⋅\n",
    "�\n",
    "θ=θ−α⋅v\n",
    "where \n",
    "�\n",
    "v is the velocity vector.\n",
    "\n",
    "AdaGrad (Adaptive Gradient Algorithm): AdaGrad adapts the learning rate for each parameter based on the historical gradients. It scales down the learning rate for frequently occurring parameters and scales up the learning rate for infrequently occurring parameters. However, AdaGrad suffers from the diminishing learning rate problem.\n",
    "\n",
    "RMSProp (Root Mean Square Propagation): RMSProp addresses the diminishing learning rate problem of AdaGrad by using an exponentially decaying average of squared gradients to normalize the learning rate.\n",
    "\n",
    "Adam (Adaptive Moment Estimation): Adam combines the advantages of RMSProp and momentum. It maintains both a decaying average of past gradients (like momentum) and a decaying average of past squared gradients (like RMSProp). Adam also includes bias correction terms to account for the initializations of the momentum and squared gradient estimates.\n",
    "\n",
    "In terms of convergence speed and memory requirements, the tradeoffs among these variants can vary. Here's a brief comparison:\n",
    "\n",
    "Convergence Speed: Techniques like momentum, RMSProp, and Adam typically converge faster compared to basic gradient descent and its variants like SGD. This is because they adaptively adjust the learning rate based on the gradients, leading to more efficient updates.\n",
    "\n",
    "Memory Requirements: Techniques like batch gradient descent require more memory as they compute gradients for the entire dataset at once. On the other hand, stochastic methods like SGD and mini-batch gradient descent use less memory since they operate on smaller subsets of the data. However, methods like Adam and RMSProp may require additional memory to store the exponentially decaying averages of gradients.\n",
    "\n",
    "Robustness to Hyperparameters: While techniques like SGD are less sensitive to hyperparameters like learning rate, momentum, and batch size, adaptive methods like AdaGrad, RMSProp, and Adam automatically adjust the learning rates based on the gradients and are less sensitive to the choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293e89f",
   "metadata": {},
   "source": [
    "# optimizer techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ca0a6",
   "metadata": {},
   "source": [
    "5.  Stochastic Gradient Descent (SGD) is a variant of gradient descent optimization commonly used in training neural networks and other machine learning models. In traditional gradient descent, the model parameters are updated after computing the gradient of the loss function over the entire training dataset. In contrast, SGD updates the parameters after computing the gradient using only a small subset of the training data, typically referred to as a mini-batch.\n",
    "\n",
    "Advantages of Stochastic Gradient Descent compared to traditional gradient descent:\n",
    "\n",
    "Efficiency: By using mini-batches instead of the entire dataset, SGD can update the parameters more frequently, leading to faster convergence. This is particularly beneficial when dealing with large datasets where computing the gradient over the entire dataset would be computationally expensive.\n",
    "\n",
    "Regularization: The stochastic nature of SGD introduces noise in parameter updates, which acts as a form of regularization. This can help prevent overfitting by adding variability to the optimization process, leading to models that generalize better to unseen data.\n",
    "\n",
    "Escape from Local Minima: The stochastic nature of SGD allows it to escape local minima more easily compared to traditional gradient descent. The noise introduced by using mini-batches can help the optimization process jump out of small valleys in the loss landscape, potentially leading to better exploration of the parameter space.\n",
    "\n",
    "Suitability for Online Learning: SGD is well-suited for scenarios where new data arrives continuously or in streams, as it can be easily adapted to update the model parameters incrementally as new data becomes available.\n",
    "\n",
    "Limitations of Stochastic Gradient Descent:\n",
    "\n",
    "Noisy Updates: The stochastic nature of SGD can introduce noise in the parameter updates, which may result in fluctuations in the training process and slower convergence compared to traditional gradient descent, especially in the initial stages of training.\n",
    "\n",
    "Need for Tuning Hyperparameters: SGD requires careful tuning of hyperparameters such as learning rate and mini-batch size. Poor choices of hyperparameters can lead to suboptimal performance or instability in the training process.\n",
    "\n",
    "Potential for Convergence Issues: Due to the randomness in the parameter updates, SGD may not converge to the optimal solution in some cases. It may oscillate around the optimal solution or get stuck in saddle points or plateaus in the loss landscape.\n",
    "\n",
    "Scenarios where Stochastic Gradient Descent is most suitable:\n",
    "\n",
    "Large Datasets: SGD is well-suited for training on large datasets where computing gradients over the entire dataset is impractical due to computational constraints.\n",
    "\n",
    "Non-Convex Optimization: SGD is effective for optimizing non-convex loss functions commonly encountered in deep learning, as it can help the optimization process escape local minima and explore the parameter space more effectively.\n",
    "\n",
    "Online Learning: SGD is suitable for scenarios where new data arrives continuously, such as in online learning settings, as it can adaptively update the model parameters with each new data point or mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d345dd1",
   "metadata": {},
   "source": [
    "6.  Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the advantages of both momentum and adaptive learning rates. It was proposed by Diederik P. Kingma and Jimmy Ba in their 2014 paper \"Adam: A Method for Stochastic Optimization\". Adam maintains both a decaying average of past gradients (like momentum) and a decaying average of past squared gradients (like RMSProp). Here's how Adam works:\n",
    "\n",
    "Initialization: Adam initializes the first and second moment variables \n",
    "�\n",
    "m and \n",
    "�\n",
    "v to zero vectors.\n",
    "\n",
    "Compute Gradients: At each iteration, Adam computes the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "Update Bias-Corrected First and Second Moment Estimates:\n",
    "\n",
    "Adam computes the first moment estimate \n",
    "�\n",
    "�\n",
    "m \n",
    "t\n",
    "​\n",
    "  (the mean of the gradients) and the second moment estimate \n",
    "�\n",
    "�\n",
    "v \n",
    "t\n",
    "​\n",
    "  (the uncentered variance of the gradients) using exponential moving averages.\n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "1\n",
    "⋅\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "1\n",
    ")\n",
    "⋅\n",
    "�\n",
    "�\n",
    "m \n",
    "t\n",
    "​\n",
    " =β \n",
    "1\n",
    "​\n",
    " ⋅m \n",
    "t−1\n",
    "​\n",
    " +(1−β \n",
    "1\n",
    "​\n",
    " )⋅g \n",
    "t\n",
    "​\n",
    " \n",
    "�\n",
    "�\n",
    "=\n",
    "�\n",
    "2\n",
    "⋅\n",
    "�\n",
    "�\n",
    "−\n",
    "1\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "2\n",
    ")\n",
    "⋅\n",
    "�\n",
    "�\n",
    "2\n",
    "v \n",
    "t\n",
    "​\n",
    " =β \n",
    "2\n",
    "​\n",
    " ⋅v \n",
    "t−1\n",
    "​\n",
    " +(1−β \n",
    "2\n",
    "​\n",
    " )⋅g \n",
    "t\n",
    "2\n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "�\n",
    "g \n",
    "t\n",
    "​\n",
    "  is the gradient at time \n",
    "�\n",
    "t, and \n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  and \n",
    "�\n",
    "2\n",
    "β \n",
    "2\n",
    "​\n",
    "  are decay rates for the first and second moments, usually close to 1 (e.g., \n",
    "�\n",
    "1\n",
    "=\n",
    "0.9\n",
    "β \n",
    "1\n",
    "​\n",
    " =0.9 and \n",
    "�\n",
    "2\n",
    "=\n",
    "0.999\n",
    "β \n",
    "2\n",
    "​\n",
    " =0.999).\n",
    "Bias Correction: Adam corrects the bias in the first and second moment estimates as they are initialized to zero, which can lead to suboptimal performance, especially during the initial iterations. The bias-corrected estimates are computed as follows:\n",
    "�\n",
    "^\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "1\n",
    "−\n",
    "�\n",
    "1\n",
    "�\n",
    "m\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " = \n",
    "1−β \n",
    "1\n",
    "t\n",
    "​\n",
    " \n",
    "m \n",
    "t\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "�\n",
    "^\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "1\n",
    "−\n",
    "�\n",
    "2\n",
    "�\n",
    "v\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " = \n",
    "1−β \n",
    "2\n",
    "t\n",
    "​\n",
    " \n",
    "v \n",
    "t\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "Update Parameters: Adam updates the parameters \n",
    "�\n",
    "θ using the bias-corrected first and second moment estimates:\n",
    "�\n",
    "�\n",
    "+\n",
    "1\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "^\n",
    "�\n",
    "+\n",
    "�\n",
    "⋅\n",
    "�\n",
    "^\n",
    "�\n",
    "θ \n",
    "t+1\n",
    "​\n",
    " =θ \n",
    "t\n",
    "​\n",
    " − \n",
    "v\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " \n",
    "​\n",
    " +ϵ\n",
    "α\n",
    "​\n",
    " ⋅ \n",
    "m\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " \n",
    "where \n",
    "�\n",
    "α is the learning rate, \n",
    "�\n",
    "ϵ is a small constant (typically \n",
    "1\n",
    "0\n",
    "−\n",
    "8\n",
    "10 \n",
    "−8\n",
    " ) to prevent division by zero, and \n",
    "�\n",
    "t represents the iteration number.\n",
    "\n",
    "Adam combines momentum and adaptive learning rates in the following ways:\n",
    "\n",
    "Momentum: Adam maintains a moving average of the gradients \n",
    "�\n",
    "�\n",
    "m \n",
    "t\n",
    "​\n",
    " , which acts as momentum and helps accelerate the optimization process by dampening oscillations.\n",
    "\n",
    "Adaptive Learning Rates: Adam adapts the learning rate for each parameter based on the magnitude of the gradients. It scales the learning rate inversely proportional to the square root of the second moment estimate \n",
    "�\n",
    "^\n",
    "�\n",
    "v\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " . This allows for larger updates for parameters with small gradients and smaller updates for parameters with large gradients.\n",
    "\n",
    "Benefits of Adam:\n",
    "\n",
    "Efficiency: Adam combines the benefits of momentum and adaptive learning rates, leading to faster convergence compared to traditional gradient descent variants.\n",
    "\n",
    "Robustness: Adam is relatively less sensitive to the choice of hyperparameters compared to other optimization algorithms, making it easier to use in practice.\n",
    "\n",
    "Applicability: Adam is widely used in various deep learning tasks and has been shown to perform well in practice across different domains.\n",
    "\n",
    "Potential Drawbacks of Adam:\n",
    "\n",
    "Memory Requirements: Adam requires additional memory to store the first and second moment estimates for each parameter, which could be a drawback in memory-constrained environments, especially when dealing with large models.\n",
    "\n",
    "Hyperparameter Tuning: While Adam is robust to the choice of hyperparameters to some extent, fine-tuning the hyperparameters (such as learning rate and decay rates) may still be necessary for optimal performance in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0811ab7",
   "metadata": {},
   "source": [
    "7.  RMSprop (Root Mean Square Propagation) is an optimization algorithm commonly used for training neural networks. It addresses the challenges of adaptive learning rates by adapting the learning rates of individual parameters based on the magnitudes of their gradients. Here's how RMSprop works and how it compares to the Adam optimizer:\n",
    "\n",
    "RMSprop:\n",
    "Algorithm:\n",
    "\n",
    "RMSprop maintains a moving average of the squared gradients for each parameter.\n",
    "It updates the parameters by dividing the gradient by the root mean square of the past gradients (hence the name RMSprop).\n",
    "The update rule for parameter \n",
    "�\n",
    "θ at time step \n",
    "�\n",
    "t is given by:\n",
    "�\n",
    "�\n",
    "+\n",
    "1\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "θ \n",
    "t+1\n",
    "​\n",
    " =θ \n",
    "t\n",
    "​\n",
    " − \n",
    "v \n",
    "t\n",
    "​\n",
    " +ϵ\n",
    "​\n",
    " \n",
    "η\n",
    "​\n",
    " ⋅g \n",
    "t\n",
    "​\n",
    " \n",
    "where:\n",
    "�\n",
    "η is the learning rate.\n",
    "�\n",
    "�\n",
    "g \n",
    "t\n",
    "​\n",
    "  is the gradient at time step \n",
    "�\n",
    "t.\n",
    "�\n",
    "�\n",
    "v \n",
    "t\n",
    "​\n",
    "  is the exponentially weighted moving average of the squared gradients.\n",
    "�\n",
    "ϵ is a small constant added for numerical stability.\n",
    "Advantages:\n",
    "\n",
    "Adaptive Learning Rates: RMSprop adapts the learning rates of individual parameters based on the magnitude of their gradients. This allows for faster convergence and better handling of sparse gradients compared to fixed learning rates.\n",
    "\n",
    "Numerical Stability: The addition of the small constant \n",
    "�\n",
    "ϵ prevents division by zero and improves numerical stability during training.\n",
    "\n",
    "Adam:\n",
    "Adam (Adaptive Moment Estimation) is another popular optimization algorithm that combines the concepts of momentum and adaptive learning rates. Here's how Adam works and how it compares to RMSprop:\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "Adam maintains two moving averages: the first moment (mean) of the gradients (similar to momentum) and the second moment (uncentered variance) of the gradients.\n",
    "It updates the parameters by computing adaptive learning rates for each parameter based on the moving averages.\n",
    "The update rule for parameter \n",
    "�\n",
    "θ at time step \n",
    "�\n",
    "t is given by:\n",
    "�\n",
    "�\n",
    "+\n",
    "1\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "^\n",
    "�\n",
    "+\n",
    "�\n",
    "⋅\n",
    "�\n",
    "^\n",
    "�\n",
    "θ \n",
    "t+1\n",
    "​\n",
    " =θ \n",
    "t\n",
    "​\n",
    " − \n",
    "v\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " \n",
    "​\n",
    " +ϵ\n",
    "η\n",
    "​\n",
    " ⋅ \n",
    "m\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    " \n",
    "where:\n",
    "�\n",
    "^\n",
    "�\n",
    "m\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    "  is the bias-corrected first moment estimate.\n",
    "�\n",
    "^\n",
    "�\n",
    "v\n",
    "^\n",
    "  \n",
    "t\n",
    "​\n",
    "  is the bias-corrected second moment estimate.\n",
    "Other symbols have the same meanings as in RMSprop.\n",
    "Advantages:\n",
    "\n",
    "Combination of Momentum and Adaptive Learning Rates: Adam combines the benefits of momentum-based optimization and adaptive learning rates, making it effective in a wide range of scenarios.\n",
    "\n",
    "Efficient: Adam typically converges faster than traditional gradient descent and other optimization algorithms due to its adaptive learning rates and momentum.\n",
    "\n",
    "Comparison:\n",
    "Strengths of RMSprop:\n",
    "\n",
    "Simplicity: RMSprop is simpler to implement compared to Adam.\n",
    "Better handling of sparse gradients: RMSprop is known to perform well in scenarios with sparse gradients.\n",
    "Weaknesses of RMSprop:\n",
    "\n",
    "Lack of momentum: RMSprop does not incorporate momentum, which can slow down convergence in certain scenarios, especially when the gradients are noisy or the loss landscape has long, narrow valleys.\n",
    "Strengths of Adam:\n",
    "\n",
    "Combines momentum and adaptive learning rates: Adam efficiently combines the benefits of momentum and adaptive learning rates, making it suitable for a wide range of optimization problems.\n",
    "\n",
    "Fast convergence: Adam typically converges faster than RMSprop, especially in scenarios with noisy or non-stationary gradients.\n",
    "\n",
    "Weaknesses of Adam:\n",
    "\n",
    "Complexity: Adam is more complex to implement compared to RMSprop, which may require more computational resources and tuning of hyperparameters.\n",
    "\n",
    "Sensitivity to hyperparameters: Adam's performance can be sensitive to the choice of hyperparameters, such as the learning rate and exponential decay rates for the moving averages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cb510",
   "metadata": {},
   "source": [
    "# Applying optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c798ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "8.   import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define training function\n",
    "def train(model, optimizer, criterion, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Define testing function\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy: %.2f %%' % (100 * correct / total))\n",
    "\n",
    "# SGD optimizer\n",
    "model_sgd = SimpleNN()\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model_sgd, optimizer_sgd, criterion)\n",
    "print(\"SGD Optimizer Test Accuracy:\")\n",
    "test(model_sgd)\n",
    "\n",
    "# Adam optimizer\n",
    "model_adam = SimpleNN()\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.001)\n",
    "train(model_adam, optimizer_adam, criterion)\n",
    "print(\"Adam Optimizer Test Accuracy:\")\n",
    "test(model_adam)\n",
    "\n",
    "# RMSprop optimizer\n",
    "model_rmsprop = SimpleNN()\n",
    "optimizer_rmsprop = optim.RMSprop(model_rmsprop.parameters(), lr=0.001)\n",
    "train(model_rmsprop, optimizer_rmsprop, criterion)\n",
    "print(\"RMSprop Optimizer Test Accuracy:\")\n",
    "test(model_rmsprop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d08c0cc",
   "metadata": {},
   "source": [
    "9.   \n",
    "When choosing the appropriate optimizer for a neural network architecture and task, several considerations and tradeoffs must be taken into account. Here are some factors to consider:\n",
    "\n",
    "Convergence Speed: Different optimizers may converge to the optimal solution at different rates. Some optimizers, like Adam, may converge faster due to their adaptive learning rates and momentum, while others, like SGD, may converge more slowly. Consider the desired training time and computational resources when selecting an optimizer.\n",
    "\n",
    "Stability: Optimizers should be stable and robust to variations in the data and model architecture. Some optimizers, such as SGD with momentum or RMSprop, can help stabilize the training process by smoothing out updates and adapting learning rates based on past gradients. Avoid optimizers that exhibit high variability in training performance or are sensitive to small changes in hyperparameters.\n",
    "\n",
    "Generalization Performance: The chosen optimizer should help the neural network generalize well to unseen data. Look for optimizers that prevent overfitting and promote better generalization by regularizing the model during training. Techniques such as adaptive learning rates, momentum, and batch normalization can contribute to improved generalization performance.\n",
    "\n",
    "Noise Sensitivity: Some optimizers may exhibit sensitivity to noise in the gradients, especially in scenarios with high variance or sparse gradients. Consider using optimizers with built-in mechanisms for handling noise, such as adaptive learning rates or momentum, to stabilize training and prevent divergence.\n",
    "\n",
    "Memory and Computational Resources: Different optimizers require varying amounts of memory and computational resources during training. For example, optimizers like Adam may require more memory due to the storage of additional moving average estimates, while simpler optimizers like SGD may be more memory-efficient. Consider the available resources and scalability requirements when selecting an optimizer.\n",
    "\n",
    "Hyperparameter Sensitivity: Optimizers often have hyperparameters that need to be tuned for optimal performance, such as learning rate, momentum, and decay rates. Some optimizers, like Adam, are less sensitive to the choice of hyperparameters compared to others. Consider the ease of tuning and the sensitivity of the optimizer to hyperparameters when selecting an appropriate optimizer.\n",
    "\n",
    "Model Architecture and Task: The choice of optimizer may depend on the specific characteristics of the neural network architecture and the nature of the task. For example, optimizers like RMSprop or Adam are commonly used for training deep neural networks due to their ability to handle non-convex loss landscapes and adapt to the varying gradients across different layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
